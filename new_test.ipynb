{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a9f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import io \n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "LLM_MODEL = \"gpt-5-2025-08-07\"\n",
    "\n",
    "# 파일 경로 설정\n",
    "raw_path = \"/home/hyunji/WEO_Chat/weo_test.xlsx\"\n",
    "structured_csv_path = \"structured_from_llm.csv\"\n",
    "hierarchical_json_path = \"extracted_hierarchical.json\"\n",
    "response_path = \"response.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_to_raw_text(path):\n",
    "    try:\n",
    "        xls = pd.ExcelFile(path)\n",
    "        all_sheets_text = \"\"\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            df = xls.parse(sheet_name, header=None)\n",
    "            all_sheets_text += f\"--- START OF SHEET: {sheet_name} ---\\n\"\n",
    "            all_sheets_text += df.to_csv(index=False, header=False)\n",
    "            all_sheets_text += f\"--- END OF SHEET: {sheet_name} ---\\n\\n\"\n",
    "        return all_sheets_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error while parsing: {e}\")\n",
    "        return None\n",
    "\n",
    "restructure_prompt = \"\"\"\n",
    "You are a data wrangling expert. You will be given text extracted from a complex, multi-sheet Excel file.\n",
    "Your task is to understand the pivoted and messy structure and convert it into a clean, normalized CSV format.\n",
    "\n",
    "Follow these rules precisely:\n",
    "1.  Identify the main data table in each sheet. Ignore all metadata, titles, comments like 'Back to contents page', and empty rows.\n",
    "2.  The columns of the output CSV must be: `Product`, `Region`, `Year`, `Scenario`, `Value`.\n",
    "3.  Unpivot the data. The original file has years and scenarios as columns. You must transform them into rows.\n",
    "4.  The 'Product' can be determined from the sheet name (e.g., 'Sheet1' for 'Renewables', 'Sheet2' for 'Total') or from context within the sheet.\n",
    "5.  The 'Scenario' is indicated in the column headers (e.g., 'Stated Policies Scenario', 'Announced Pledges Scenario').\n",
    "6.  The 'Year' is also in the column headers. Match the correct year and scenario to each value.\n",
    "7.  The 'Region' is in one of the first few columns of the data rows.\n",
    "8.  Output ONLY the CSV data, including a header row. Do not include any explanations, comments, or markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "print(\"1단계: LLM으로 비정형 엑셀 데이터 정제 시작...\")\n",
    "raw_excel_text = excel_to_raw_text(raw_path)\n",
    "\n",
    "if raw_excel_text:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": restructure_prompt},\n",
    "            {\"role\": \"user\", \"content\": raw_excel_text}\n",
    "        ]\n",
    "    )\n",
    "    structured_csv_str = response.choices[0].message.content.strip()\n",
    "\n",
    "    if structured_csv_str.startswith(\"```csv\"):\n",
    "        structured_csv_str = structured_csv_str[len(\"```csv\"):].strip()\n",
    "    if structured_csv_str.endswith(\"```\"):\n",
    "        structured_csv_str = structured_csv_str[:-len(\"```\")].strip()\n",
    "\n",
    "    with open(structured_csv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(structured_csv_str)\n",
    "    print(f\"LLM이 정제된 CSV 생성 완료! 파일 저장: {structured_csv_path}\")\n",
    "else:\n",
    "    print(\"엑셀 파일 읽기 실패.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f204a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# --- 3. 2단계: 규칙 기반으로 정형 CSV를 계층적 JSON으로 변환 ---\n",
    "\n",
    "def create_hierarchical_json_from_csv(csv_data_string):\n",
    "    try:\n",
    "        df = pd.read_csv(io.StringIO(csv_data_string))\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 데이터를 읽는 중 오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "    structured_data = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            product = row['Product']\n",
    "            region = row['Region']\n",
    "            year = int(row['Year'])\n",
    "            scenario = row['Scenario']\n",
    "            value = float(row['Value'])\n",
    "            \n",
    "            structured_data[region][year][scenario][product] = value\n",
    "        except (KeyError, ValueError, TypeError) as e:\n",
    "            print(f\"행 처리 중 오류 발생 (건너뜀): {row}, 오류: {e}\")\n",
    "            continue\n",
    "            \n",
    "    return json.loads(json.dumps(structured_data))\n",
    "\n",
    "print(\"\\n2단계: 규칙 기반으로 CSV를 계층적 JSON으로 변환 시작...\")\n",
    "hierarchical_data = create_hierarchical_json_from_csv(structured_csv_str)\n",
    "\n",
    "if hierarchical_data:\n",
    "    with open(hierarchical_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(hierarchical_data, json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"계층적 JSON 변환 완료! 파일 저장: {hierarchical_json_path}\")\n",
    "else:\n",
    "    print(\"JSON 변환 실패.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23044c1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6392a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a meticulous and insightful data analyst specializing in global energy trends.\n",
    "Your task is to answer the user's question based exclusively on the provided JSON data context.\n",
    "\n",
    "Follow these instructions carefully to structure your response:\n",
    "\n",
    "1.  **Analyze the User's Query:** Understand if the user is asking for a specific fact, a comparison, a calculation, or an inference based on the data.\n",
    "\n",
    "2.  **Ground Your Answer in Data:** All parts of your response must be directly supported by the provided JSON data. Do not use any external knowledge.\n",
    "\n",
    "3.  **Perform Necessary Operations:**\n",
    "    -   For direct questions, find the exact value.\n",
    "    -   For comparison questions, retrieve all relevant values and compare them.\n",
    "    -   For calculation questions (e.g., percentage, difference), state the formula you used and the values you plugged in.\n",
    "\n",
    "4.  **Handle Inferences Carefully:**\n",
    "    -   If the user asks for an opinion or inference (e.g., \"potential risks\"), you are allowed to make a logical deduction based on the patterns and numbers in the data.\n",
    "    -   You MUST explicitly state that this is an inference. For example, start your reasoning with \"Based on the data showing a low renewables share of X% and high total consumption of Y EJ, a logical inference is that...\".\n",
    "\n",
    "5.  **Format Your Output:** Your final output must be a single, valid JSON object with the following three keys: \"answer\", \"reference\", \"reasoning\".\n",
    "    -   `\"answer\"`: Provide a direct and concise answer to the user's question.\n",
    "    -   `\"reference\"`: Cite the specific data points from the context that support your answer, weaving them into a readable sentence or paragraph.\n",
    "    -   `\"reasoning\"`: Explain the step-by-step logic you used to arrive at the answer. Describe any comparisons or calculations performed.\n",
    "\"\"\"\n",
    "\n",
    "def process_user_query(user_question: str, data_context: dict) -> dict:\n",
    "    print(f\"\\n'{user_question}' 질문 처리 시작...\")\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"User Question: \\\"{user_question}\\\"\\n\\nJSON Data:\\n{json.dumps(data_context)}\"}\n",
    "            ]\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        parsed_response = json.loads(content)\n",
    "        print(\"질문 처리 완료!\")\n",
    "        return parsed_response\n",
    "\n",
    "    except openai.APIError as e:\n",
    "        print(f\"OpenAI API 오류 발생: {e}\")\n",
    "        return {\"error\": \"API Error\", \"details\": str(e)}\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"JSON 파싱 오류 발생. LLM이 유효한 JSON을 반환하지 않았습니다.\")\n",
    "        return {\"error\": \"JSON Decode Error\", \"raw_content\": content}\n",
    "    except Exception as e:\n",
    "        print(f\"알 수 없는 오류 발생: {e}\")\n",
    "        return {\"error\": \"Unknown Error\", \"details\": str(e)}\n",
    "\n",
    "user_queries = [\n",
    "    \"What is the Total Energy of Japan in 2050, in the APS scenario?\",\n",
    "    \"In 2030 under the Stated Policies Scenario, what is the renewables' share of total energy in the United States?\",\n",
    "    \"Based on the 2050 data, which region appears most vulnerable in a global energy transition due to a combination of high total energy consumption and a low share of renewables? Explain the potential risks based on the data.\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(user_queries):\n",
    "    result = process_user_query(query, hierarchical_data)\n",
    "    \n",
    "    print(f\"\\n--- 결과 (질문 {i+1}) ---\")\n",
    "    print(json.dumps(result, indent=4, ensure_ascii=False))\n",
    "    \n",
    "    if i == 0:\n",
    "        with open(response_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\n(첫 번째 질문의 답변을 {response_path} 파일에 저장했습니다.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43933d7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
